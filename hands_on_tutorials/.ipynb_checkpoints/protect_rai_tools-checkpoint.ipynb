{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1659639f-a9e2-438c-b297-fed922ab1bd0",
   "metadata": {},
   "source": [
    "# Tools to understand the behavior of ML algorithms - Hands-on tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067eb9fc-4199-4ac3-af63-20bf2c75aa17",
   "metadata": {},
   "source": [
    "In this notebook, we provide step-by-step guides for geting started using 2 Microsoft Responsible AI techniques for protecting AI systems data:\n",
    "\n",
    "- **Data Anonymization** with [Presidio](https://github.com/Microsoft/presidio)\n",
    "- **Differencial Privacy** with the [SmartNoise system](https://github.com/opendp/smartnoise-core)\n",
    "\n",
    "These are inspired by the example notebooks provided in each repo, and are meant to get aggregate all these tools into a single document to get you started as quickly as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00defb99-5be6-48da-826d-92aa360603cd",
   "metadata": {},
   "source": [
    "## Presidio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2225023-965d-48f4-907e-7bb4bd7d4f9e",
   "metadata": {},
   "source": [
    "Presidio is a data protection and anonymization SDK for text and images providing fast identification and anonymization of private entities in text such as credit card numbers, names, locations, social security numbers, bitcoin wallets, US phone numbers, financial data and more.\n",
    "\n",
    "In this tutorial, we introduce two of Presidio's modules, namely \n",
    "1. **Presidio analyzer** for custom or predefined PII detection in text, leveraging Named Entity Recognition, regular expressions, rule-based logic, and checksum with relevant context in multiple languages, and \n",
    "2. **Presidio anonymizer** which is the module allowing anonymization of the detected PII entities using different operators. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa36abd-410a-4056-86ae-01cfeca4bb86",
   "metadata": {},
   "source": [
    "To get there, we follow 4 simple steps to show how Presidio works.\n",
    "\n",
    "**Step 1:** Installing the presidio_analyzer and presidio_anonymizer libraries using pip along with the spaCy English language model needed by the analyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf267da1-4fdb-4725-9f87-1a35c7486bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing packages if not already done\n",
    "# !pip install presidio_analyzer \n",
    "# !pip install presidio_anonymizer\n",
    "\n",
    "# Presidio analyzer requires a spaCy language model. \n",
    "# !python -m spacy download en_core_web_lg\n",
    "\n",
    "# Importing the required modules\n",
    "from presidio_analyzer import AnalyzerEngine, PatternRecognizer\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_anonymizer.entities.engine import OperatorConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a18767c-591e-45fb-aa46-6e63da2bb041",
   "metadata": {},
   "source": [
    "**Step 2:** Once the presidio-analyzer package is installed, run this simple analysis script. This will print the result of the PII analysis, in this case the detected phone numbers in the provided text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3119e682-5256-4536-b582-977ca36f8032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-07-19 13:41:26,573] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'His'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-07-19 13:41:26,577] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'name'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-07-19 13:41:26,579] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'is'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-07-19 13:41:26,581] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'Mr.'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-07-19 13:41:26,582] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'Jones'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-07-19 13:41:26,584] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'and'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-07-19 13:41:26,585] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'his'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-07-19 13:41:26,586] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'phone'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-07-19 13:41:26,588] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'number'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-07-19 13:41:26,589] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'is'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-07-19 13:41:26,591] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '212'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-07-19 13:41:26,592] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '-'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-07-19 13:41:26,593] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '555'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-07-19 13:41:26,595] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '-'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-07-19 13:41:26,598] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '5555'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[type: PHONE_NUMBER, start: 46, end: 58, score: 1.0]\n"
     ]
    }
   ],
   "source": [
    "text_to_anonymize = \"His name is Mr. Jones and his phone number is 212-555-5555\"\n",
    "analyzer = AnalyzerEngine()\n",
    "analyzer_results = analyzer.analyze(text=text_to_anonymize, entities=[\"PHONE_NUMBER\"], language='en')\n",
    "\n",
    "print(analyzer_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ff134e-bc90-47cb-a9cd-7722e954d023",
   "metadata": {},
   "source": [
    "**Step 3:**  Creating Custom PII Entity Recognizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebdba327-fdc1-49ad-a131-c808fd3bb3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-07-19 13:48:34,249] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'His'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-07-19 13:48:34,251] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'name'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-07-19 13:48:34,253] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'is'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-07-19 13:48:34,254] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'Mr.'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-07-19 13:48:34,255] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'Jones'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-07-19 13:48:34,256] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'and'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-07-19 13:48:34,257] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'his'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-07-19 13:48:34,258] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'phone'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-07-19 13:48:34,259] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'number'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-07-19 13:48:34,260] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'is'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-07-19 13:48:34,261] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '212'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-07-19 13:48:34,262] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '-'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-07-19 13:48:34,263] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '555'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-07-19 13:48:34,264] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '-'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-07-19 13:48:34,266] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '5555'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[type: TITLE, start: 12, end: 15, score: 1.0,\n",
       " type: PRONOUN, start: 26, end: 29, score: 1.0,\n",
       " type: PHONE_NUMBER, start: 46, end: 58, score: 1.0,\n",
       " type: PERSON, start: 16, end: 21, score: 0.85]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from presidio_analyzer import PatternRecognizer\n",
    "\n",
    "text_to_anonymize = \"His name is Mr. Jones and his phone number is 212-555-5555\" \n",
    "titles_recognizer = PatternRecognizer(supported_entity=\"TITLE\",\n",
    "                                      deny_list=[\"Mr.\",\"Mrs.\",\"Miss\"])\n",
    "pronoun_recognizer = PatternRecognizer(supported_entity=\"PRONOUN\",\n",
    "                                      deny_list=[\"he\", \"his\", \"she\", \"hers\"])\n",
    "analyzer.registry.add_recognizer(titles_recognizer)\n",
    "analyzer.registry.add_recognizer(pronoun_recognizer)\n",
    "\n",
    "analyzer_results = analyzer.analyze(text=text_to_anonymize, language='en')\n",
    "\n",
    "analyzer_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b412feb6-ba4f-4331-a3c5-6129d55b91a4",
   "metadata": {},
   "source": [
    "The previous code sample:\n",
    "1. Creates custom titles and pronouns recognizers.\n",
    "2. Adds the new custom recognizers to the analyzer.\n",
    "3. Calls analyzer to get results from the old and new recognizers.\n",
    "\n",
    "It prints all the PII detected including titles and pronouns we just defined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91723dbd-5fea-4b72-bcb2-cae7441bb3bc",
   "metadata": {},
   "source": [
    "**Step 4:**  Anonymizing the identified PII entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5653d198-67db-42da-833d-04a2360a20c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"text\": \"His name is  <ANONYMIZED> and <ANONYMIZED> phone number is ************\", \"items\": [{\"start\": 59, \"end\": 71, \"entity_type\": \"PHONE_NUMBER\", \"text\": \"************\", \"operator\": \"mask\"}, {\"start\": 30, \"end\": 42, \"entity_type\": \"PRONOUN\", \"text\": \"<ANONYMIZED>\", \"operator\": \"replace\"}, {\"start\": 13, \"end\": 25, \"entity_type\": \"PERSON\", \"text\": \"<ANONYMIZED>\", \"operator\": \"replace\"}, {\"start\": 12, \"end\": 12, \"entity_type\": \"TITLE\", \"text\": \"\", \"operator\": \"redact\"}]}'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_anonymizer.entities.engine import OperatorConfig\n",
    "\n",
    "\n",
    "anonymizer = AnonymizerEngine()\n",
    "\n",
    "\n",
    "anonymized_results = anonymizer.anonymize(\n",
    "    text=text_to_anonymize,\n",
    "    analyzer_results=analyzer_results,    \n",
    "    operators={\"DEFAULT\": OperatorConfig(\"replace\", {\"new_value\": \"<ANONYMIZED>\"}), \n",
    "               \"PHONE_NUMBER\": OperatorConfig(\"mask\", {\"type\": \"mask\", \"masking_char\" : \"*\", \"chars_to_mask\" : 12, \"from_end\" : True}),\n",
    "                \"TITLE\": OperatorConfig(\"redact\", {})}\n",
    ")\n",
    "\n",
    "anonymized_results.to_json()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1dd937-a9a7-4ff2-8100-f7774588685d",
   "metadata": {},
   "source": [
    "The previous code sample:\n",
    "1. Sets up the anonymizer engine.\n",
    "2. Creates an anonymizer request - text to anonymize, list of anonymizers to apply and the results from the analyzer request.\n",
    "3. Anonymizes the text.\n",
    "\n",
    "It prints the anonymized text along with a list of the detected PII entities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60990d0c-486c-432c-8519-303645159576",
   "metadata": {},
   "source": [
    "## SmartNoise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646a0abf-89fe-4d23-9cce-dc2e53253685",
   "metadata": {},
   "source": [
    "**SmartNoise** is a joint project by Microsoft and Harvard's Institute for Quantitative Social Science (IQSS) and the School of Engineering and Applied Sciences (SEAS) as part of the OpenDP initiative. It aims to make Differential Privacy broadly accessible.\n",
    "\n",
    "The SmartNoise tools primarily focus on the \"global model\" of **Differential Privacy** where a trusted data collector is presumed to have access to unprotected data and wishes to protect public releases of aggregate information. For example, a hospital having access to patientsâ€™ information and wishing to release aggregated statistics about these patients without affecting their privacy.\n",
    "\n",
    "SmartNoise is an open-source project that contains different components for building global differentially private systems. SmartNoise is made up of a core library and an SDK, only the [SmartNoise core library](https://github.com/opendp/smartnoise-core) is explored here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3130dffc-e76b-4bd7-a155-317a58c0c5e9",
   "metadata": {},
   "source": [
    "In this tutorial, we will explore how data can be protected against reidentification using Differential Privacy and the SmartNoise system.\n",
    "\n",
    "The goal is to show how an attacker can leverage basic demographic information like age and zip codes to reidentify individuals even when the sensitive data is published in an anonymized format. Then we show how Differential Privacy can help prevent such an attack. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4479a116-4920-47a4-bfcd-f7b506fba8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries, uncomment if needed\n",
    "# %pip install git+https://github.com/opendifferentialprivacy/smartnoise-sdk#subdirectory=sdk\n",
    "# %pip install faker zipcodes tqdm opendp-smartnoise\n",
    "# !pip install z3-solver==4.8.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30854a3e-16b2-42ab-a678-1e57d24669d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import uuid\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import reident_tools as reident\n",
    "from opendp.smartnoise.synthesizers.mwem import MWEMSynthesizer\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
